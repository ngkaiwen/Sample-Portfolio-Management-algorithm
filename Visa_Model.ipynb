{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visa Stock Predictive Model\n",
    "\"Refer to Appendix B of the report for instructions on how to obtain prediction values from our model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief structure of our notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating features:\n",
    "+ Seasonality removal and detrending\n",
    "+ Signal processing\n",
    "+ Raw log returns\n",
    "+ Combining all the feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating and selecting features\n",
    "- Hybrid feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "- Keras neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Install 2 necessary libraries for our model\n",
    "_ = !pip install keras\n",
    "_ = !pip install pandas_datareader #This module allows us to read data from yahoo finance\n",
    "_ = !pip install seasonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import basic data processing and plotting modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_datareader import data\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "#Import Keras - A tensorflow wrapper that will allow us to experiment with NNs easily\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import regularizers\n",
    "from keras.layers import advanced_activations as reLu\n",
    "\n",
    "#Import modules used to extract seasonality and select features \n",
    "import statsmodels.api as sm\n",
    "from pylab import rcParams\n",
    "from seasonal import fit_seasons, adjust_seasons\n",
    "from sklearn.feature_selection import (RFE,RFECV)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
    "from sklearn.svm import (SVR,LinearSVR)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful data structures\n",
    "Below, we define a number of useful variables and data structures that enable our model to be easily adapted to the various stocks we are predicting. By simply making changes to the variables below, all the changes propogate to the entire notebook. This means that we do not have to go through the laborious process of changing multiple variable names when predicting different stocks. The following features were selected due to extensive research that showed they were likely to correlate with the stock due to reasons such as being competitors, clients, suppliers, being in the same industry or an industry that is affected by the industry the stock we are trying to predict is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#List out the trackers of the stocks you want to download here (check yahoo finance to get the tracker)\n",
    "TRACKER_LIST = [\"V\",\"MA\",\"AXP\",\"PYPL\",\"BAC\",\"HAWK\",\"BR\",\"PAY\",\"DFS\",\"FDC\",\"JPM\",\"WFC\"] #List out the trackers of the stocks you want to download here (check yahoo finance to get the tracker)\n",
    "NAMES = {\n",
    "    \"V\": \"Visa Inc.\",\n",
    "  \"MA\":\"Mastercard Incorporated\",\n",
    "  \"AXP\":\"American Express Company\",\n",
    "  \"PYPL\":\"Paypal Holdings Inc\",\n",
    "  \"BAC\":\"Bank of America\",\n",
    "  \"HAWK\":\"Blackhawk Network Holdings Inc\",\n",
    "  \"BR\":\"Broadridge Financial Solution\",\n",
    "  \"PAY\":\"VeriFone Systems Inc\",\n",
    "  \"DFS\":\"Discover Financial Services\",\n",
    "  \"FDC\":\"First Data Corpation\",\n",
    "  \"JPM\":\"JPMorgan Chase & Co.\",\n",
    "  \"WFC\":\"Wells Fargo & Company\",   \n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Shift data dictionary to help our algorithm automatically shift stocks in the later part of our code\n",
    "\"shift_data\" contains information on whether a particular stock should be shifted or not. \n",
    "\"0\" indicates that it should not be shifted, while \"1\" indicates that it should be.\n",
    "\"\"\"\n",
    "shift_data = {\n",
    "  \"V\":1,\n",
    "  \"MA\":1,\n",
    "  \"AXP\":1,\n",
    "  \"PYPL\":1,\n",
    "  \"BAC\":1,\n",
    "  \"HAWK\":1,\n",
    "  \"BR\":1,\n",
    "  \"PAY\":1,\n",
    "  \"DFS\":1,\n",
    "  \"FDC\":1,\n",
    "  \"JPM\":1,\n",
    "  \"WFC\":1,\n",
    "}\n",
    "\n",
    "#We define this variable so that we don't need to make multiple changes for each of our models\n",
    "#ie. we are able to quickly adapt our jupyter notebook to different stocks which we are buying\n",
    "STOCK_TO_PREDICT = \"V\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although many of the companies here have rather long histories, accuracy seemed to work best when we limited training data to 2013 onwards, possibily due to the constantly evolving payments industry, due to reasons such as new entrants and new technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source = \"yahoo\"\n",
    "START = \"2013-6-15\" #Start date for download\n",
    "END = \"2017-11-17\" #End date for download\n",
    "\n",
    "stocks = data.DataReader(TRACKER_LIST,source,START,END).ix[\"Close\"] #This downloads the data from yahoo and formats it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stocks.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All datafields with NA is filled up with data from previous day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stocks = stocks.fillna(method=\"ffill\")\n",
    "stocks.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = stocks.plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seaonality Removal and Detrending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why stationarity of data is important\n",
    "Stationarity of time series data is important because, in its absence, a model describing the data will vary in accuracy at different time points. As such, stationarity is required for sample statistics such as means, variances, and correlations to accurately describe the data at all time points of interest. Therefore, it is crucial for us to transform our dataset such that it is stationary. There are a few ways to achieve this, including:\n",
    "1. Detrending data\n",
    "2. Deseasonalizing data\n",
    "\n",
    "Current literature advances the notion that \"if neural networks use deseasonalized time series then the neural networks can focus on learning trend and cyclical components\" and thus predict better. Thus, we attempt below to introduce deseasonalized data to our model.\n",
    "\n",
    "Since the TSA decomposition function allows for both the removal of trend and seasonality, our deseasonalising and detrending steps will be done concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the use of deseasonalised data \n",
    "The Stats Model's TSA.seasonal_decompose function returns data with the exact same number of data points as the raw data. The additional features can thus be used alongside the daily stock prices. While the additive model is useful when the seasonal variation is relatively constant over time, it returns negative values which makes it hard to apply log returns on them. Thus, after experimentation, we decided to go with the multiplicative model and log-additive model.\n",
    "\n",
    "As can be seen in the charts below, seasonality on daily data is not very meaningful for both multiplicative and log-additive models. We experimented by adding the deseasonalised features into our model. However, they did not correlate well with the original stock's returns and did little to improve the accuracy of the model.\n",
    "\n",
    "This can be explained by the charts itself. While there is a slight linear upward trend, the plot of trend data mirrors the observed data. The seasonality charts also deviates from seasonality trends present for the beauty industry where new products are released in anticipation for different seasons. The graphs show that the output did not accurately reflect all real world considerations. This could be explained by the tool used which only made used of moving average to decompose the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deseason = pd.DataFrame()\n",
    "deseason = stocks.index.copy()\n",
    "deseason =stocks[STOCK_TO_PREDICT].copy()\n",
    "deseason = deseason.fillna(deseason.bfill())\n",
    "rcParams['figure.figsize'] = 7.5, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutliplicative deseasonalisation\n",
    "Below, we experimented with different freq parameters. Since we are unaware of the intrinsic seasonality of the data, we experimented with 4 different freq values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decomposition2 = sm.tsa.seasonal_decompose(deseason, model=\"multiplicative\", freq=7) \n",
    "decomposition2.plot() \n",
    "plt.show()\n",
    "decomposition3 = sm.tsa.seasonal_decompose(deseason, model=\"multiplicative\", freq=14) \n",
    "decomposition3.plot() \n",
    "plt.show() \n",
    "decomposition4 = sm.tsa.seasonal_decompose(deseason, model=\"multiplicative\", freq=21) \n",
    "decomposition4.plot() \n",
    "plt.show()\n",
    "decomposition5 = sm.tsa.seasonal_decompose(deseason, model=\"multiplicative\", freq=28) \n",
    "decomposition5.plot() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-Additive Model\n",
    "\n",
    "Log additive model is used as it can convert a multiplicative model into an additive model.\n",
    "- ln(Xt ) = ln (Tt x St x Rt ) \n",
    "- ln(Xt ) = ln(Tt) + ln(St) + ln(Rt)\n",
    "\n",
    "This can possibly help with the problem of negative values from using the purely additive model. We will add this as additional features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scale = pd.DataFrame()\n",
    "scale[STOCK_TO_PREDICT] = (stocks[STOCK_TO_PREDICT]/max(stocks[STOCK_TO_PREDICT]))\n",
    "\n",
    "deseasonLog = pd.DataFrame()\n",
    "deseasonLog = stocks.index.copy()\n",
    "\n",
    "log = pd.DataFrame()\n",
    "log[STOCK_TO_PREDICT] = np.log(scale[STOCK_TO_PREDICT]/scale[STOCK_TO_PREDICT].shift())\n",
    "deseasonLog =log[STOCK_TO_PREDICT].copy()\n",
    "deseasonLog = deseasonLog.fillna(deseasonLog.bfill())\n",
    "rcParams['figure.figsize'] = 7, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decomposition9 = sm.tsa.seasonal_decompose(deseasonLog, model=\"additive\", freq=7) \n",
    "decomposition9.plot() \n",
    "plt.show()\n",
    "decomposition10 = sm.tsa.seasonal_decompose(deseasonLog, model=\"additive\", freq=14) \n",
    "decomposition10.plot() \n",
    "plt.show() \n",
    "decomposition11 = sm.tsa.seasonal_decompose(deseasonLog, model=\"additive\", freq=21) \n",
    "decomposition11.plot() \n",
    "plt.show()\n",
    "decomposition12 = sm.tsa.seasonal_decompose(deseasonLog, model=\"additive\", freq=28) \n",
    "decomposition12.plot() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Processing for Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize the robustness of ANN and combine it with signal processing techniques to propose a predictive model for stock market indices.\n",
    "Signal processing is concerned with processing one and multi-dimensional time series via a spectrum of techniques, filters and algorithms. We are using the Gaussian Zero-Phase Shift filter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are basing our model on the paper titled \"STOCK MARKET PREDICTIVE MODEL BASED ON INTEGRATION OF SIGNAL PROCESSING AND ARTIFICIAL NEURAL NETWORK\" by Dinesh K. Sharma and Aaron R. Rababaah from University of Maryland. In this paper, log returns is not used, as the filters are sufficient enough to create accurate data that can be used in our NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Zero-Phase Shift Filter (GZ-filter)\n",
    "Gaussian Zero-Phase Shift filter (GZ-filter) is a very effective digital signal filter. Rababaah (2009) used the GZ-filter to smooth the output of the surveillance situation tracking signal which may be prone to human and random errors. Therefore, we propose the GZ- filter to be used as the smoothing model of the stock market index signal as the first step before we apply further processing such as segmentation, feature vector modeling, training and classification, etc. The concept of GZ-filter is to estimate the signal level at each time instance by computing a kernel of Gaussian-weighted neighborhood of the surrounding data points. \n",
    "\n",
    "GZ(S(t), d(t)) = Sum(G(t)S(t)) from t-d(t) to t+d(t)\n",
    "where S(t): the input noisy signal, t: time, d(t): time interval considered to compute the de-noised signal, G(t): Gaussian probability distribution function.\n",
    "\n",
    "The uniqueness of the GZ-filter is that unlike conventional signal filters such as moving average or median filter, it retains the phase of the original signal. This means the peaks of the original signal are guaranteed to align with the filtered signal which could be critical if one is interested to identify the exact instances where the signal have abrupt changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d\n",
    "smooth = pd.DataFrame()\n",
    "\n",
    "for i in stocks:\n",
    "  smooth[i] = gaussian_filter1d(stocks[i], 3)\n",
    "\n",
    "_ = smooth.plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The x-axis is in units of time. As it can be seen, the Gaussian Zero-Phase Shift Filter has smoothened the curve while retaining the phase of the original signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signal Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the data is normalized to the range [0, 1]. This is important since the Artificial Neural Network – Multi-Layered Perceptron (ANN- MLP) model requires normalized input vectors. Mathematically, the normalization is expressed as:\n",
    "\n",
    "S(i) = S(i)/max(S), where, S(i) is the signal level at index i and S is the entire signal and max is the maximum level of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import concat\n",
    "\n",
    "scaled_smooth_noDate = pd.DataFrame()\n",
    "scaled_smooth_data = pd.DataFrame()\n",
    "intermediate_data = pd.DataFrame()\n",
    "\n",
    "for column in smooth:\n",
    "      scaled_smooth_noDate[column] = smooth[column]/max(np.nanmax(smooth[column]))\n",
    "    \n",
    "index = stocks.index\n",
    "scaled_smooth_data = scaled_smooth_noDate.set_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(scaled_smooth_data[STOCK_TO_PREDICT].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the new scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = scaled_smooth_data.plot(figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the features of the other stocks more clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log returns is used so that our NN model will be only using 2 classes (increase/decrease), as opposed to the paper's original 20 classes of price levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_returns = pd.DataFrame()\n",
    "\n",
    "for column in scaled_smooth_data:\n",
    "  log_returns[column] = np.log(scaled_smooth_data[column]/scaled_smooth_data[column].shift())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = log_returns.plot(figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if signal processing makes the data stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to test if the data is stationary we used the Augmented Dickey Fuller Test. From the test data, we can be assured that the data set is now stationary as the p value is extremely low (below 5% or 1%), this means that the null hypothesis of the process having no unit root can be rejected. This means that the time series has no time-dependent structure. Therefore we can safely use this method to make our time series stationary. Thus, even without using detrending or deseasonalizing the data, signal processing can be used in place of detrending and deseasonalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "resid = log_returns[STOCK_TO_PREDICT].values\n",
    "\n",
    "#remove empty values which will throw problems during the test\n",
    "x = resid[~np.isnan(resid)] \n",
    "\n",
    "result = adfuller(x)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "\tprint('\\t%s: %.4f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at autocorrelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "\n",
    "fig_2 = plt.figure()\n",
    "fig_2.set_figwidth(20)\n",
    "fig_2.set_figheight(15)\n",
    "\n",
    "for column in log_returns:\n",
    "  autocorrelation_plot(log_returns[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalise signal processed data\n",
    "Below, we take the final step of shifting our signal processed features to prepare them for our neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "signal_processed_shifted = pd.DataFrame()\n",
    "\n",
    "#We then shift all the data according to their respective time zones\n",
    "for stock in log_returns:\n",
    "  if shift_data[stock] == 1:\n",
    "    signal_processed_shifted[stock] = log_returns[stock].shift() #Shift stocks in the same time zone as the stock to be predicted by 1 day\n",
    "  else:\n",
    "    signal_processed_shifted[stock] = log_returns[stock] #Don't shift the other stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add raw log returns (without signal processing smoothening)\n",
    "We also add the \"vanilla\" raw log returns of each stock to create additional features for our NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_log_returns = pd.DataFrame()\n",
    "\n",
    "for stock in stocks:\n",
    "  raw_log_returns[stock + \"_raw\"] = np.log(stocks[stock]/stocks[stock].shift())\n",
    "  \n",
    "for stock in raw_log_returns:\n",
    "  if shift_data[stock[:-4]] == 1:\n",
    "    raw_log_returns[stock] = raw_log_returns[stock].shift() #Shift stocks in the same time zone as the stock to be predicted by  1 day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine both signal processed and raw log return data\n",
    "We combine the two dataframes which were computed above to obtain our final feature set for our neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shifted_returns = pd.merge(raw_log_returns, signal_processed_shifted, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shifted_returns = shifted_returns.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add classification labels (ie. whether log returns are positive or negative)\n",
    "\n",
    "Below, we obtain the classification label for our stock separately and add it into our dataframe. This label is what our neural network model will be trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stock_to_predict_log_returns = pd.DataFrame()\n",
    "stock_to_predict_log_returns[\"temp_stock_to_predict\"] = np.log(stocks[STOCK_TO_PREDICT]/stocks[STOCK_TO_PREDICT].shift())\n",
    "\n",
    "#Obtain the labels\n",
    "stock_to_predict_log_returns[\"STOCK_RISES\"] = 0\n",
    "stock_to_predict_log_returns.ix[stock_to_predict_log_returns[\"temp_stock_to_predict\"] >=0, \"STOCK_RISES\"] = 1\n",
    "stock_to_predict_log_returns = stock_to_predict_log_returns.drop(\"temp_stock_to_predict\",axis=1)\n",
    "\n",
    "#Add label column into the shifted_returns dataframe\n",
    "shifted_returns = pd.merge(shifted_returns, stock_to_predict_log_returns, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the correlations between the features and the stock to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#shifted_returns.corr()[STOCK_TO_PREDICT + \"_raw\"] #We drop the label column here because it's irrelevant to determining if the features are correlated with the stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shifted_returns.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from using the default correlation function, there are many are ways to help us determine which features are most important. Including too many features can actually worsen accuracy as there is less can mean more noise and thus less pattern for the neural network to learn from.As such, we use an Hybrid Feature Selection strategy similar to the one covered in the reading.We record the score of all the features in terms of how useful it was to predict our target variable from a variety of different methods. The mean score then used to determine the overall usefulness of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = shifted_returns[STOCK_TO_PREDICT + \"_raw\"].values\n",
    "X3 = shifted_returns.drop(['STOCK_RISES'],axis=1)\n",
    "X2 = X3.drop([STOCK_TO_PREDICT + \"_raw\"],axis=1)\n",
    "X = X3.as_matrix()\n",
    "colnames = X2.columns\n",
    "# Define dictionary to store our rankings\n",
    "ranks = {}\n",
    "# Create our function which stores the feature rankings to the ranks dictionary\n",
    "def ranking(ranks, names, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x,2), ranks)\n",
    "    return dict(zip(names, ranks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest was indicated as the most useful classifier in the paper, and we have included that. However, several of the feature selectors discussed in the paper do not seem that useful after our own experimentation, such as Bayesian methods, and as such we have selected some others of our own.\n",
    "\n",
    "Linear Regression is one such method. We also implemented this a second way by using it in conjunction with a tool in Sklearn known as Recursive Feature Elimination or RFE.RFE uses a model such as Linear Regression to select either the best or worst-performing feature, and then excludes this feature. The whole process is then iterated until all features in the dataset are used up.Support Vector Regression, which is characterized by usage of kernels, absence of local minima, sparseness of the solution and capacity control obtained by acting on the margin, or on number of support vectors, is also used with RFE. \n",
    "\n",
    "Ridge regression,which performs ‘L2 regularization‘, i.e. it adds a factor of sum of squares of coefficients in the optimization objective, is also used, along with the default correlation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using Random Forest\n",
    "rf = RandomForestRegressor(n_jobs=-1, n_estimators=50, verbose=3)\n",
    "rf.fit(X,Y)\n",
    "ranks[\"RF\"] = ranking(rf.feature_importances_, colnames)\n",
    "\n",
    "#Run Default Correlation Function\n",
    "ranks[\"Corr\"] = shifted_returns.corr()[STOCK_TO_PREDICT + \"_raw\"]\n",
    "#for i in colnames:\n",
    "#    ranks[\"Corr\"][i] = round(ranks[\"Corr\"][i], 4)\n",
    "\n",
    "# Construct our Linear Regression model and rank features using Recursive Feature Elimination\n",
    "lr = LinearRegression(normalize=True)\n",
    "lr.fit(X,Y)\n",
    "#stop the search when only the last feature is left\n",
    "rfe = RFE(lr, n_features_to_select=1, verbose =3 )\n",
    "rfe.fit(X,Y)\n",
    "ranks[\"RFE\"] = ranking(list(map(float, rfe.ranking_)), colnames, order=-1)\n",
    "\n",
    "#rfe2 using Support Vector Regression and RFE\n",
    "svr = LinearSVR()\n",
    "rfe2 = RFE(estimator=svr, n_features_to_select=1, step=1)\n",
    "rfe2.fit(X, Y )\n",
    "ranks[\"RFE2\"] = ranking(list(map(float, rfe2.ranking_)), colnames, order=-1)\n",
    "\n",
    "# Using Linear Regression\n",
    "lr = LinearRegression(normalize=True)\n",
    "lr.fit(X,Y)\n",
    "ranks[\"LinReg\"] = ranking(np.abs(lr.coef_), colnames)\n",
    "\n",
    "# Using Ridge \n",
    "ridge = Ridge(alpha = 7)\n",
    "ridge.fit(X,Y)\n",
    "ranks['Ridge'] = ranking(np.abs(ridge.coef_), colnames)\n",
    "\n",
    "# Create empty dictionary to store the mean value calculated from all the scores\n",
    "r = {}\n",
    "for name in colnames:\n",
    "    r[name] = round(np.mean([ranks[method][name] \n",
    "                             for method in ranks.keys()]), 2)\n",
    "\n",
    "methods = sorted(ranks.keys())\n",
    "ranks[\"Mean\"] = r\n",
    "methods.append(\"Mean\")\n",
    " \n",
    "print(\"\\t%s\" % \"\\t\".join(methods))\n",
    "for name in colnames:\n",
    "    print(\"%s\\t%s\" % (name, \"\\t\".join(map(str, \n",
    "                         [ranks[method][name] for method in methods]))))\n",
    "# Put the mean scores into a Pandas dataframe\n",
    "meanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])\n",
    "\n",
    "# Sort the dataframe\n",
    "meanplot = meanplot.sort_values('Mean Ranking', ascending=False)\n",
    "\n",
    "# Let's plot the ranking of the features\n",
    "sns.factorplot(x=\"Mean Ranking\", y=\"Feature\", data = meanplot, kind=\"bar\", \n",
    "               size=10, aspect=1.9, palette='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above information, we then decide how many features we want to keep by setting a minimum threshold for the mean score of each feature, anything lower than this figure will be removed by the function below. Based on experimentation 0.2 seems to allow the optimal number of features that do help to improve the metrics like accuracy subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numfeatures = len(colnames)\n",
    "print(ranks[\"Mean\"])\n",
    "def dropfeatures(data,colnames,threshold):\n",
    "    for i in colnames:\n",
    "        if ranks[\"Mean\"][i] <= threshold:\n",
    "            print(i+\" dropped\")\n",
    "            data = data.drop([i],axis=1)\n",
    "    return data\n",
    "shifted_returns = dropfeatures(shifted_returns,colnames,0.2)\n",
    "shifted_returns.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create our labels dataframe, which contains the labels (whether the stock rises or falls) for both training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pd.DataFrame()\n",
    "labels = shifted_returns.loc[:,[\"STOCK_RISES\"]] #Obtain labels of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define some of the key variables we are going to use to split our data into 3 parts:\n",
    "- Number of days of testing data that we want to use\n",
    "- Number of days of training data that we want to use\n",
    "- Number of days to predict (typically 1 day, ie. the next day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Change these parameters according to the number of days of testing data you want to use and the number of features you're placing into keras\n",
    "\n",
    "DAYS_TO_PREDICT = 1\n",
    "LENGTH_TESTING = int(0.25*len(labels))\n",
    "LENGTH_TRAINING = len(labels) - LENGTH_TESTING\n",
    "NUM_OF_FEATURES = len(shifted_returns.columns)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = shifted_returns.drop(\"STOCK_RISES\",axis=1).values\n",
    "y = labels.values.reshape((len(labels), 1)) #Change pandas dataframe to a numpy array for use with Keras\n",
    "\n",
    "TRAINING_DATA_DAYS = -LENGTH_TESTING - DAYS_TO_PREDICT\n",
    "x_train = x[:TRAINING_DATA_DAYS]\n",
    "y_train = y[:TRAINING_DATA_DAYS]\n",
    "\n",
    "x_test = x[TRAINING_DATA_DAYS : -DAYS_TO_PREDICT]\n",
    "y_test = y[TRAINING_DATA_DAYS : -DAYS_TO_PREDICT]\n",
    "\n",
    "x_to_predict = x[-DAYS_TO_PREDICT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, two metric functions is defined as required by Keras: precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our model. The current model uses a four layer NN with 20,50,20 and 50 neurons respectively.Increasing the number beyond these layers or neurons seem to cause potential overfitting til to the lower resulting accuracy. Usage of adam has also been more effective when coupled with feature selection for more significant improvements.Regularisation has had marginal benefit, especially using l2 instead of l1, and the current degree of regularisation seems to work best for this model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#LeakyRelu tried, no diff in results\n",
    "model.add(Dense(20, activation = reLu.LeakyReLU(alpha=0.3), input_dim=NUM_OF_FEATURES, activity_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(50, activation = reLu.LeakyReLU(alpha=0.3),activity_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(20, activation = reLu.LeakyReLU(alpha=0.3),activity_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(50, activation = reLu.LeakyReLU(alpha=0.3),activity_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "model.add(Dense(1,activation = \"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy',precision, recall])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the neural model using the training data defined above. Epochs ranging from 50 to 250 were tested, as well as batch sizes from 5 to 30 before deciding on the current number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          epochs=200,\n",
    "          batch_size=20, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a quick check to see that our model has a reasonable level of accuracy, precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now output the vital test results from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.metrics_names)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally use the model to make a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_to_predict = x_to_predict.reshape(1,NUM_OF_FEATURES) #Reshape the data so that it fits the input structure of our model\n",
    "result = model.predict(x_to_predict)\n",
    "\n",
    "print(\"Probability that stock will rise in the next day: \" + str(result[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
